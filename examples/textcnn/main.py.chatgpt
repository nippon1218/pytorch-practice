import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import AG_NEWS
from torchtext.data import Field, BucketIterator
from torchtext.data.utils import get_tokenizer
import time

# 定义设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 定义文本处理工具：Tokenize 和转换为 Tensor
tokenizer = get_tokenizer("spacy")  # 使用 spacy tokenizer

TEXT = Field(sequential=True, tokenize=tokenizer, lower=True)
LABEL = Field(sequential=False, use_vocab=False, is_target=True)

# 加载数据集
train_data, test_data = AG_NEWS(split=('train', 'test'))

# 构建词汇表
TEXT.build_vocab(train_data, max_size=10000, min_freq=1)

# 创建迭代器
train_iter, test_iter = BucketIterator.splits(
    (train_data, test_data),
    batch_size=64,
    device=device,
    sort_within_batch=True,
    sort_key=lambda x: len(x.text),
)

# 定义一个简单的文本分类模型
class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_classes, dropout=0.5):
        super(TextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.conv1 = nn.Conv1d(embed_size, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(embed_size, 128, kernel_size=4, padding=2)
        self.conv3 = nn.Conv1d(embed_size, 128, kernel_size=5, padding=2)
        self.fc = nn.Linear(128*3, num_classes)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(0, 2, 1)  # (batch_size, embed_size, seq_length)

        # 卷积层
        conv1_out = torch.relu(self.conv1(x)).max(dim=2)[0]
        conv2_out = torch.relu(self.conv2(x)).max(dim=2)[0]
        conv3_out = torch.relu(self.conv3(x)).max(dim=2)[0]

        # 合并卷积层输出
        x = torch.cat([conv1_out, conv2_out, conv3_out], dim=1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# 初始化模型、损失函数、优化器
vocab_size = len(TEXT.vocab)
embed_size = 128
num_classes = 4  # AG_NEWS 是一个 4 类问题
model = TextCNN(vocab_size, embed_size, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练过程
def train_model(model, train_iter, optimizer, criterion, num_epochs=5):
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        epoch_accuracy = 0
        start_time = time.time()
        
        for batch in train_iter:
            text, label = batch.text, batch.label
            text, label = text.to(device), label.to(device)

            optimizer.zero_grad()
            output = model(text)
            loss = criterion(output, label)
            loss.backward()
            optimizer.step()

            # 计算损失和准确度
            epoch_loss += loss.item()
            _, predicted = output.max(1)
            epoch_accuracy += (predicted == label).sum().item()

        epoch_loss /= len(train_iter)
        epoch_accuracy /= len(train_iter.dataset)

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy*100:.2f}%")
        print(f"Epoch Time: {time.time() - start_time:.2f}s")

# 测试过程
def evaluate_model(model, test_iter):
    model.eval()
    test_accuracy = 0
    with torch.no_grad():
        for batch in test_iter:
            text, label = batch.text, batch.label
            text, label = text.to(device), label.to(device)

            output = model(text)
            _, predicted = output.max(1)
            test_accuracy += (predicted == label).sum().item()

    test_accuracy /= len(test_iter.dataset)
    print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# 训练并评估模型
train_model(model, train_iter, optimizer, criterion, num_epochs=5)
evaluate_model(model, test_iter)
